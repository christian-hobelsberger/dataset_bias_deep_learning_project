{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_stats(guessed_labels, true_labels):    \n",
    "    label_count = len(guessed_labels)\n",
    "    negative_instance_count = label_count - sum(true_labels)\n",
    "    \n",
    "    threshold = np.sort(guessed_labels[:,0])[negative_instance_count]\n",
    "    \n",
    "    rounded_guessed_labels = np.zeros(label_count)\n",
    "    \n",
    "    print(\"there are:\", sum(true_labels), \"car images in the testing dataset, which has a total size of:\", len(true_labels))\n",
    "        \n",
    "    for idx in range(label_count):\n",
    "        if guessed_labels[idx] >= threshold:\n",
    "            rounded_guessed_labels[idx] = 1\n",
    "        else:\n",
    "            rounded_guessed_labels[idx] = 0\n",
    "            \n",
    "\n",
    "    f1 = metrics.f1_score(rounded_guessed_labels, true_labels) \n",
    "    recall = metrics.recall_score(rounded_guessed_labels, true_labels)   \n",
    "    precision = metrics.precision_score(rounded_guessed_labels, true_labels)  \n",
    "    accuracy = metrics.accuracy_score(rounded_guessed_labels, true_labels)\n",
    "    print(sum(rounded_guessed_labels), \"images were labelled as a car image\")\n",
    "\n",
    "    print('f1 score, precision, recall, mean precision-recall', 'accuracy')\n",
    "    return(np.array([f1, precision, recall, 0.5*(precision+recall), accuracy]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Caltech 101 data\n",
    "\n",
    "# Load and preprocess the images and labels\n",
    "data_dir = 'data/caltech101/caltech-101/101_ObjectCategories'\n",
    "\n",
    "\n",
    "\n",
    "caltech_images = []\n",
    "caltech_labels = []\n",
    "for folder_name in os.listdir(data_dir):\n",
    "    folder_path = os.path.join(data_dir, folder_name)\n",
    "    if folder_name == 'car_side':\n",
    "        label = 1\n",
    "    else:\n",
    "        label = 0\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        img = image.load_img(file_path, target_size=(224, 224))\n",
    "        img_array = image.img_to_array(img)\n",
    "        caltech_images.append(img_array)\n",
    "        caltech_labels.append(label)\n",
    "\n",
    "# Convert lists to arrays\n",
    "caltech_images = np.array(caltech_images)\n",
    "caltech_labels = np.array(caltech_labels)\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "caltech_X_train, caltech_X_val, caltech_y_train, caltech_y_val = train_test_split(caltech_images, caltech_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load VOC data\n",
    "\n",
    "# Load and preprocess the images and labels\n",
    "\n",
    "# Directory paths\n",
    "data_dir = 'data/VOC2007'\n",
    "images_dir = os.path.join(data_dir, 'JPEGImages')\n",
    "annotations_dir = os.path.join(data_dir, 'Annotations')\n",
    "\n",
    "# Function to parse XML annotation files and extract object names\n",
    "def parse_annotation(xml_file):\n",
    "    import xml.etree.ElementTree as ET\n",
    "    tree = ET.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    "    objects = []\n",
    "    for obj in root.findall('object'):\n",
    "        obj_name = obj.find('name').text\n",
    "        objects.append(obj_name)\n",
    "    return objects\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Load and preprocess the images and labels\n",
    "voc_images_list = []\n",
    "voc_labels_list = []\n",
    "\n",
    "for filename in os.listdir(images_dir):\n",
    "    if filename.endswith('.jpg'):\n",
    "        image_path = os.path.join(images_dir, filename)\n",
    "        annotation_path = os.path.join(annotations_dir, filename[:-4] + '.xml')  # Replace extension with .xml\n",
    "\n",
    "        if os.path.exists(annotation_path):\n",
    "            objects = parse_annotation(annotation_path)\n",
    "            if 'car' in objects:  # Check if 'car' object is present\n",
    "                label = 1\n",
    "            else:\n",
    "                label = 0\n",
    "\n",
    "            try:\n",
    "                img = image.load_img(image_path, target_size=(224, 224))\n",
    "                img_array = image.img_to_array(img)\n",
    "                voc_images_list.append(img_array)\n",
    "                voc_labels_list.append(label)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading image {image_path}: {e}\")\n",
    "\n",
    "# Convert lists to arrays\n",
    "voc_images = np.array(voc_images_list)\n",
    "voc_labels = np.array(voc_labels_list)\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "voc_X_train, voc_X_val, voc_y_train, voc_y_val = train_test_split(voc_images, voc_labels, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load msrc data\n",
    "\n",
    "# Load and preprocess the images and labels\n",
    "data_dir = 'data/msrcorid'\n",
    "\n",
    "msrc_images_list = []\n",
    "msrc_labels_list = []\n",
    "\n",
    "\n",
    "processed_files = set()\n",
    "\n",
    "def process_folder(folder_path, label):\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        if os.path.isdir(file_path):\n",
    "            process_folder(file_path, label)  # Recursively process subdirectories\n",
    "        else:\n",
    "            # Check if file has already been processed\n",
    "            if file_path not in processed_files:\n",
    "                processed_files.add(file_path)  # Add file to processed set\n",
    "                try:\n",
    "                    img = image.load_img(file_path, target_size=(224, 224))\n",
    "                    img_array = image.img_to_array(img)\n",
    "                    msrc_images_list.append(img_array)\n",
    "                    msrc_labels_list.append(label)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading image {file_path}: {e}\")\n",
    "\n",
    "# Loop through top-level directories\n",
    "for folder_name in os.listdir(data_dir):\n",
    "    folder_path = os.path.join(data_dir, folder_name)\n",
    "    if os.path.isdir(folder_path):\n",
    "        if folder_name == 'cars':\n",
    "            label = 1\n",
    "        else:\n",
    "            label = 0\n",
    "        # Process the folder\n",
    "        process_folder(folder_path, label)\n",
    "\n",
    "# Convert lists to arrays\n",
    "msrc_images = np.array(msrc_images_list)\n",
    "msrc_labels = np.array(msrc_labels_list)\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "msrc_X_train, msrc_X_val, msrc_y_train, msrc_y_val = train_test_split(msrc_images, msrc_labels, test_size=0.2, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "caltech_model = load_model('models/caltech101_model_cnn_adj.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc_model = load_model('models/voc2007_model_cnn_adj.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "msrc_model = load_model('models/msrcorid_model_cnn_adj.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 187ms/step\n",
      "\u001b[1m286/286\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 325ms/step\n",
      "\u001b[1m286/286\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 248ms/step\n"
     ]
    }
   ],
   "source": [
    "tr_caltech_te_caltech_results = caltech_model.predict(caltech_X_val)\n",
    "tr_voc_te_caltech_results = voc_model.predict(caltech_images)\n",
    "tr_msrc_te_caltech_results = msrc_model.predict(caltech_images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 459ms/step\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 173ms/step\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 222ms/step\n"
     ]
    }
   ],
   "source": [
    "tr_caltech_te_voc_results = caltech_model.predict(voc_images)\n",
    "tr_voc_te_voc_results = voc_model.predict(voc_X_val)\n",
    "tr_msrc_te_voc_results = msrc_model.predict(voc_images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 263ms/step\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 171ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 179ms/step\n"
     ]
    }
   ],
   "source": [
    "tr_caltech_te_msrc_results = caltech_model.predict(msrc_images)\n",
    "tr_voc_te_msrc_results = voc_model.predict(msrc_images)\n",
    "tr_msrc_te_msrc_results = msrc_model.predict(msrc_X_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print results of predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are: 25 car images in the testing dataset, which has a total size of: 1829\n",
      "129.0 images were labelled as a car image\n",
      "f1 score, precision, recall, mean precision-recall accuracy\n",
      "[0.25974026 0.8        0.15503876 0.47751938 0.93767086]\n",
      "there are: 123 car images in the testing dataset, which has a total size of: 9145\n",
      "881.0 images were labelled as a car image\n",
      "f1 score, precision, recall, mean precision-recall accuracy\n",
      "[0.10358566 0.42276423 0.05902384 0.24089403 0.90158557]\n",
      "there are: 123 car images in the testing dataset, which has a total size of: 9145\n",
      "123.0 images were labelled as a car image\n",
      "f1 score, precision, recall, mean precision-recall accuracy\n",
      "[0.08943089 0.08943089 0.08943089 0.08943089 0.97550574]\n"
     ]
    }
   ],
   "source": [
    "print(return_stats(tr_caltech_te_caltech_results, caltech_y_val))\n",
    "print(return_stats(tr_voc_te_caltech_results, caltech_labels))\n",
    "print(return_stats(tr_msrc_te_caltech_results, caltech_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are: 761 car images in the testing dataset, which has a total size of: 5011\n",
      "5011.0 images were labelled as a car image\n",
      "f1 score, precision, recall, mean precision-recall accuracy\n",
      "[0.26368676 1.         0.1518659  0.57593295 0.1518659 ]\n",
      "there are: 155 car images in the testing dataset, which has a total size of: 1003\n",
      "155.0 images were labelled as a car image\n",
      "f1 score, precision, recall, mean precision-recall accuracy\n",
      "[0.36774194 0.36774194 0.36774194 0.36774194 0.80458624]\n",
      "there are: 761 car images in the testing dataset, which has a total size of: 5011\n",
      "761.0 images were labelled as a car image\n",
      "f1 score, precision, recall, mean precision-recall accuracy\n",
      "[0.24704336 0.24704336 0.24704336 0.24704336 0.77130313]\n"
     ]
    }
   ],
   "source": [
    "print(return_stats(tr_caltech_te_voc_results, voc_labels))\n",
    "print(return_stats(tr_voc_te_voc_results, voc_y_val))\n",
    "print(return_stats(tr_msrc_te_voc_results, voc_labels))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are: 505 car images in the testing dataset, which has a total size of: 4323\n",
      "659.0 images were labelled as a car image\n",
      "f1 score, precision, recall, mean precision-recall accuracy\n",
      "[0.         0.         0.         0.         0.73074254]\n",
      "there are: 505 car images in the testing dataset, which has a total size of: 4323\n",
      "595.0 images were labelled as a car image\n",
      "f1 score, precision, recall, mean precision-recall accuracy\n",
      "[0.44181818 0.48118812 0.40840336 0.44479574 0.857969  ]\n",
      "there are: 122 car images in the testing dataset, which has a total size of: 865\n",
      "122.0 images were labelled as a car image\n",
      "f1 score, precision, recall, mean precision-recall accuracy\n",
      "[0.91803279 0.91803279 0.91803279 0.91803279 0.97687861]\n"
     ]
    }
   ],
   "source": [
    "print(return_stats(tr_caltech_te_msrc_results, msrc_labels))\n",
    "print(return_stats(tr_voc_te_msrc_results, msrc_labels))\n",
    "print(return_stats(tr_msrc_te_msrc_results, msrc_y_val))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "py312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
